{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MMA 2022S 865, Individual Assignment 1\n",
    "\n",
    "Version 2: Updated January 1, 2022.\n",
    "\n",
    "- Usharani Vankara\n",
    "- 20258698\n",
    "- Section 1\n",
    "- 26/02/2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1 - ELI5\n",
    "\n",
    "_“If you can't explain it simply, you don't understand it well enough.” – Albert Einstein_\n",
    "\n",
    "Explaining technical concepts to a non-technical audience is an underappreciated skill; one which the GMMA/MMA program aims to give its students; and one that will truly set you apart in the job market. The only way to gain a skill is by practice, so here we go.\n",
    "\n",
    "Answer each question below as though you were talking to a 5 year old (equivalently: a grandma, or a completely non-technical manager, or an Ivey grad). Use your own words. Use analogies where possible. Examples are better than theory. Keep it short, but be complete. Use simple, plain English. Do not use business buzzwords like _actualize, empower, fungible, leverage, or synergize_. Do not use technical buzzwords that most people don’t know like _model, agile, bandwidth, IoT, blockchain, AR, VR, actionable insights_. Inform the audience without going into too much technical detail. Your goal is to truly help them understand, not to give what you feel is a “technically precise” answer and move on (but they still don’t understand!). Don’t be that guy!\n",
    "\n",
    "Please keep each answer to 1000 characters or less.\n",
    "\n",
    "Finally, feel free to use [Markdown syntax](https://www.markdownguide.org/basic-syntax/) to format your answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: What is “Big Data” and how is it different than “regular data”?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "•\tBig Data is a term that refers to huge amounts of organised and unstructured data that cannot be stored, processed and analyzed using traditional methods, requiring instead \"massively parallel software running on thousands of servers\".\n",
    "\n",
    "•\tBig data is massive (Volume). While standard data is measured in megabytes, gigabytes, and terabytes, big data is measured in petabytes and zettabytes. Big data gives the architecture for dealing with this type of data. It would be impossible to mine for insights without adequate storage and processing options.\n",
    "\n",
    "•\tTraditional data is structured data that is generally stored by all sizes of businesses, from small to large. In traditional database systems, a centralised database design is used to store and maintain data in a predefined structure or fields in a file. To maintain and retrieve data, Structured Query Language (SQL) is employed.\n",
    "\n",
    "•\tWith big data, (velocity) refers to how quickly data is coming in. Sensor data from health devices is a great example. With the IoT taking off at a dramatic rate, we will be seeing more and more connected sensors. The ability to instantly process health data can provide users and physicians with potentially life-saving information.\n",
    "\n",
    "•\tApproximately 95 percent of all big data is unstructured, which means it does not easily fit into a basic, traditional paradigm. Emails and videos, as well as scientific and meteorological data, can all be part of a large data stream, each with their own distinct characteristics (Variety).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: What is Hadoop? Hint: What problems in previous data storage and processing was Hadoop designed to solve? How did Hadoop accomplish that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop is an open source platform developed by Apache that is used to store, process, and analyse massive amounts of data, often known as Big Data. Rather than storing and processing data on a single huge computer, Hadoop enables for the clustering of numerous computers to analyse massive datasets in parallel more quickly. It provides an efficient framework for performing jobs over numerous cluster nodes which is essentially a set of systems that are linked together through LAN.  Hadoop will enable us to process large volumes of data in a relatively short period of time. It was built to carry out transformations and procedures where the data resides.\n",
    "Hadoop is designed to handle the three V’s of Big Data: \n",
    "1.\tVolume: Hadoop was built to scale out, thus expanding the system is considerably more cost effective. All we have to do when we need extra storage or compute capacity is add more nodes to the cluster.\n",
    "2.\tVariety: Hadoop allows us to store data in any type, whether structured or unstructured. Hadoop is fast, adaptable, and capable of handling whatever sort of analysis we wish to perform.\n",
    "3.\tVelocity: With Hadoop, we can load raw data into the system and then decide how we want to view it later. Since data is always changing, the flexibility of the system makes it much easier to integrate any changes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: How does Big Data and the cloud help Machine Learning? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As more businesses embrace the ability to store, analyse, and extract value from massive amounts of data, it is becoming increasingly difficult for them to use the obtained data in the most effective manner. Data is a boon for machine learning systems. The more data a system receives, the more it learns to function better for businesses. Big data analytics may help make sense of data by identifying trends and patterns. With the aid of decision-making algorithms, machine learning can speed up this process. It may categorise incoming data, identify trends, and transform the data into useful insights for company operations. Effective big data management systems increase machine learning by providing analytics teams with vast amounts of high-quality, relevant data required to train such models successfully.\n",
    "\n",
    "Netflix, for example, employs machine learning algorithms to better understand individual customers' watching interests and then make better suggestions, which helps to keep people on its streaming platform for longer.\n",
    "Cloud storage has made it simpler to compute, analyse, and process large data; formerly, enterprises had to host the data on their own servers, which caused various challenges for data owners such as quality, availability, security, higher prices, maintenance, and so on.\n",
    "\n",
    "There are three primary ways in which machine learning on the cloud will benefit businesses:\n",
    "1.\tCost efficiency: The cloud's pay-per-use approach reduces the need for businesses to invest in time-consuming and costly machine learning systems that they will not utilise on a daily basis. The power of GPUs may be harnessed without the need for expensive equipment.\n",
    "2.\tWith Google Cloud Platform, Microsoft Azure, and AWS, artificial intelligence features can be implemented without requiring any deep or hardcore knowledge. The SDKs and APIs are already provided so machine learning functionalities can be directly embedded.\n",
    "3.\tEasy to scale up: Enterprises may use machine learning on the cloud to test and launch smaller initiatives on the cloud, then scale up as need and demand grows. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: What is NoSQL?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NoSQL is a type of database management system (DBMS) that does not adhere to all of the principles of a relational DBMS and cannot query data using standard SQL. It is translated as \"Not Only SQL,\" because this sort of database is not typically a replacement for RDBMSs and SQL, but rather a complementing addition to them.\n",
    "\n",
    "NoSQL-based systems are generally employed in very big databases, which are especially vulnerable to performance issues due to SQL constraints and the relational paradigm of databases. \n",
    "Traditional RDBMSs simply do not meet this need because they can only \"scale up,\" or increase the resources on a central server. A NoSQL implementation, on the other hand, can \"scale out,\" or distribute the database load across more servers.\n",
    "NoSQL databases are focused on specific classes of problems, such as being more flexible with stored data (document stores), targeting use cases such as relationships (graph databases) and data aggregation (column databases), or simply reducing the concept of a database to something that stores a value (key/value stores).\n",
    "\n",
    "When compared to RDBMSs, NoSQL databases have the advantages of quick scalability, substantially superior performance, and a simpler structure. However, because they are a relatively new and unproven technology, they cannot provide the full reporting and analytical features of RDBMS.\n",
    "The benefit of utilising a NoSQL database is that you don't have to know precisely how your data will appear before you start. The absence of SQL is one of the main disadvantages of NoSQL databases. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: Name three ways topic modeling could help a bank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic modelling is an unsupervised machine learning approach that can scan a collection of documents, find word and phrase patterns within them, and automatically cluster word groupings and related expressions that best represent a collection of documents.\n",
    "\n",
    "1.\tTopic Modelling with financial news: Topic Modeling facilitates the identification and grouping of relevant publications obtained from CNBC, Wall Street Journal and the Reuters. We can extract Trade Signals, discover key themes in business disclosures or earnings call transcripts, as well as customer assessments or contracts that have been annotated using techniques like sentiment analysis or direct labelling with subsequent asset returns.\n",
    "\n",
    "2.\tRoute conversations to the right teams based on topic: Rather than attempting to determine who should talk with the client, a topic modelling tool may tag discussions and then route them to the most relevant team utilising workflows. A communication containing the phrases “credit card”, \"mortgage/loan difficulties”, “checks/funds bouncing,” or “dispute”, for example, might be sent to the appropriate departments for settlement.\n",
    "\n",
    "3.\tTopic modelling of AppStore reviews: Banks and financial organisations have  made mobile applications available to their consumers on Google Play, Apple App Store, and Windows Phone Store. These app shops not only allow users to download software, but also allow them to submit comments and reviews. We can leverage the Google Play Store evaluations for a bank’s net banking mobile app and inspect comments for interface (better UI), quality, new features (a direct credit card payment to any bank, easier debit card blocking functionality), Issues such as spammy advertisements on login etc. We can also automate routing of topics & reviews to relevant teams (strategy, product owner, application maintenance) for further action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 6: What is Apache Spark, exactly, and what are its pros and cons?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Spark:\n",
    "It is a robust open-source distributed computation engine that may be utilised for a wide range of workloads. That is, it can run across several servers in a unified fashion, and it can receive distributed data sets and process them using code written to run within the Spark framework (\"engine\"). It may be used as a library to build data applications or to do interactive data analysis.\n",
    "\n",
    "Pros:\n",
    "1.\tSpeed: For large-scale data processing, Spark is 100 times quicker than Hadoop. To store data, Apache Spark employs an in-memory (RAM) processing environment, whereas Hadoop use a local memory area. Spark can process several petabytes of clustered data from over 8000 nodes at the same time. Its more readable also and easy to understand.\n",
    "2.\tEase of Use: Apache Spark carries easy-to-use APIs in Java, Scala, Python, R, and SQL for operating on large datasets. It offers over 80 high-level operators that make it easy to build parallel apps and it is highly configurable. Spark may also operate on a laptop, Hadoop, Apache Mesos, on its own, or in the cloud. It can connect to several data sources such as HDFS, Apache Cassandra, Apache HBase, and S3.\n",
    "3.\tComplex Analytics: Spark not only supports “MAP” and “reduce”. Spark is the foundation for a number of libraries, including SQL, DataFrames, and Datasets, as well as MLlib for machine learning, GraphX for graph processing, and Spark Streaming. These libraries can be used in tandem in the same application. \n",
    "\n",
    "Cons:\n",
    "1.\tApache Spark lacks an automatic code optimization mechanism, we must optimise the code manually. \n",
    "2.\tApache Spark does not provide a file management system of its own. It is dependent on other systems, such as Hadoop or other cloud-based platforms.\n",
    "3.\tIn the case of Apache Spark Machine Learning Spark MLlib, there are fewer algorithms.\n",
    "4.\tWindow Criteria: In Apache Spark, data is divided into tiny batches based on a predetermined time period. As a result, Apache will not support record-based window criteria. It instead provides time-based window criteria.\n",
    "5.\tApache Spark is not capable of supporting more concurrent users.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Sentiment Analysis via the ML-based approach\n",
    "\n",
    "Download the “Product Sentiment” dataset from the course portal: sentiment_train.csv and sentiment_test.csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.a. Loading and Prep\n",
    "\n",
    "Load, clean, and preprocess the data as you find necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "#NLP\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#ML Classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2400 entries, 0 to 2399\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Sentence  2400 non-null   object\n",
      " 1   Polarity  2400 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 37.6+ KB\n",
      "None\n",
      "                                            Sentence  Polarity\n",
      "0                           Wow... Loved this place.         1\n",
      "1                                 Crust is not good.         0\n",
      "2          Not tasty and the texture was just nasty.         0\n",
      "3  Stopped by during the late May bank holiday of...         1\n",
      "4  The selection on the menu was great and so wer...         1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "path = \"D:/MMA - Smith School of Business/9 Big Data Analytics - MMA 865/Assignments/Individual/\"\n",
    "df_train = pd.read_csv(path + \"sentiment_train.csv\")\n",
    "\n",
    "print(df_train.info())\n",
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for duplicate sentences\n",
    "df_train.duplicated(subset=[\"Sentence\"]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the duplicates\n",
    "df_train = df_train.drop_duplicates(subset=[\"Sentence\"],keep=\"last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentence    0\n",
       "Polarity    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for missing values\n",
    "df_train.isnull().sum()\n",
    "#There are no missing vakues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2382.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.493703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.500065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Polarity\n",
       "count  2382.000000\n",
       "mean      0.493703\n",
       "std       0.500065\n",
       "min       0.000000\n",
       "25%       0.000000\n",
       "50%       0.000000\n",
       "75%       1.000000\n",
       "max       1.000000"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 1206, 1: 1176})\n"
     ]
    }
   ],
   "source": [
    "#Check for the distribution of target variable\n",
    "from collections import Counter\n",
    "print(Counter(df_train['Polarity']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Polarity')"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train.Polarity.value_counts().plot(kind='bar')\n",
    "\n",
    "# naming the x axis\n",
    "plt.xlabel('Polarity')\n",
    "#There is no imbalance in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train = df_train['Sentence']\n",
    "y_train = df_train['Polarity']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ushai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ushai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.b. Modeling\n",
    "\n",
    "Use your favorite ML algorithm to train a classification model.  Don’t forget everything that we’ve learned in our ML course: hyperparameter tuning, cross validation, handling imbalanced data, etc. Make reasonable decisions and try to create the best-performing classifier that you can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "#stopwords = stopwords.words('english')\n",
    "def preprocess(X_train, y_train):\n",
    "    indx = X_train[X_train.duplicated()].index\n",
    "    X_train = X_train.drop(indx)\n",
    "    y_train = y_train.drop(indx)\n",
    "    X_train = X_train.reset_index(drop=True)\n",
    "    y_train = y_train.reset_index(drop=True)\n",
    "    for i in range(0,len(X_train)):\n",
    "        X_train[i] = X_train[i].lower()\n",
    "        #remove spacial characters\n",
    "        X_train[i] = re.sub('[^a-zA-Z]', ' ', X_train[i])\n",
    "        X_train[i] = X_train[i].lower()\n",
    "        #remove words with numbers\n",
    "        #X_train[i] = re.sub(r'\\d+', \"\", X_train[i]).strip()\n",
    "        X_train[i] = ' '.join(e.lower() for e in X_train[i].split() if e.lower() not in stopwords.words('english'))\n",
    "        X_train[i] = ' '.join(lemmatizer.lemmatize(e) for e in X_train[i].split() )       \n",
    "        \n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = preprocess(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                      wow loved place\n",
       "1                                           crust good\n",
       "2                                  tasty texture nasty\n",
       "3    stopped late may bank holiday rick steve recom...\n",
       "4                           selection menu great price\n",
       "Name: Sentence, dtype: object"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(5)\n",
    "#y_train.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the hyper parameters\n",
    "lr_params = {\n",
    "\n",
    "'lr__solver' :('newton-cg', 'lbfgs', 'liblinear'),\n",
    "'lr__penalty': [ 'l2'],\n",
    "'lr__class_weight':('balanced',None),\n",
    "'lr__C' : [0.001,0.01,0.1, 1, 5, 10,20],\n",
    "'lr__max_iter' : (500, 1000)\n",
    "}\n",
    "\n",
    "gb_params = {\n",
    "'vect__ngram_range': ((1, 1), (1, 2), (1, 3)),\n",
    "'tfidf__use_idf': (True, False),\n",
    "'tfidf__norm': ('l1', 'l2'),\n",
    "'clf__n_estimators': (50,100, 200)\n",
    "}\n",
    "\n",
    "tf_params = {\n",
    "'tvec__ngram_range': [(1, 1), (1, 2)],\n",
    "'tvec__stop_words': ['english'],\n",
    "}\n",
    "\n",
    "\n",
    "rf_params = {\n",
    "'tvec__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "'tvec__stop_words': ['english'],\n",
    "'rf__max_depth': [100, 500,  1000],\n",
    "'rf__max_leaf_nodes': [None], \n",
    "'rf__n_estimators' : [50, 100, 200],\n",
    "'rf__min_samples_split': [50, 100, 200],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Pipelines\n",
    "#Logistic Regression\n",
    "lr_pipe = Pipeline([\n",
    "('vect', CountVectorizer()),\n",
    "('tfidf', TfidfTransformer()),\n",
    "('lr', LogisticRegression())\n",
    "])\n",
    "#Gradient Boosting\n",
    "gb_pipe = Pipeline([\n",
    "('vect', CountVectorizer()),\n",
    "('tfidf', TfidfTransformer()),\n",
    "('clf', GradientBoostingClassifier()),\n",
    "])\n",
    "# NB pipeline setup\n",
    "nb_pipe = Pipeline([\n",
    "('tvec', TfidfVectorizer()),\n",
    "('mb', MultinomialNB())\n",
    "])\n",
    "# Randomforest pipeline setup\n",
    "rf_pipe = Pipeline([\n",
    "('tvec', TfidfVectorizer()),\n",
    "('rf', RandomForestClassifier())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['C', 'class_weight', 'dual', 'fit_intercept', 'intercept_scaling', 'l1_ratio', 'max_iter', 'multi_class', 'n_jobs', 'penalty', 'random_state', 'solver', 'tol', 'verbose', 'warm_start'])"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogisticRegression().get_params().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 84 candidates, totalling 840 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10,\n",
       "             estimator=Pipeline(steps=[('vect', CountVectorizer()),\n",
       "                                       ('tfidf', TfidfTransformer()),\n",
       "                                       ('lr', LogisticRegression())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'lr__C': [0.001, 0.01, 0.1, 1, 5, 10, 20],\n",
       "                         'lr__class_weight': ('balanced', None),\n",
       "                         'lr__max_iter': (500, 1000), 'lr__penalty': ['l2'],\n",
       "                         'lr__solver': ('newton-cg', 'lbfgs', 'liblinear')},\n",
       "             scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_lr = GridSearchCV(lr_pipe, lr_params, n_jobs=-1, verbose=1, cv = 10, scoring = \"f1\")\n",
    "grid_search_lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 36 candidates, totalling 360 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10,\n",
       "             estimator=Pipeline(steps=[('vect', CountVectorizer()),\n",
       "                                       ('tfidf', TfidfTransformer()),\n",
       "                                       ('clf', GradientBoostingClassifier())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'clf__n_estimators': (50, 100, 200),\n",
       "                         'tfidf__norm': ('l1', 'l2'),\n",
       "                         'tfidf__use_idf': (True, False),\n",
       "                         'vect__ngram_range': ((1, 1), (1, 2), (1, 3))},\n",
       "             scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_GB = GridSearchCV(gb_pipe, gb_params, n_jobs=-1, verbose=1, cv = 10, scoring = \"f1\")\n",
    "grid_search_GB.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up GridSearch for Randomforest\n",
    "grid_search_rf = GridSearchCV(rf_pipe, param_grid=rf_params,  verbose = 1, n_jobs= -1, cv = 10, scoring = \"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 81 candidates, totalling 810 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10,\n",
       "             estimator=Pipeline(steps=[('tvec', TfidfVectorizer()),\n",
       "                                       ('rf', RandomForestClassifier())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'rf__max_depth': [100, 500, 1000],\n",
       "                         'rf__max_leaf_nodes': [None],\n",
       "                         'rf__min_samples_split': [50, 100, 200],\n",
       "                         'rf__n_estimators': [50, 100, 200],\n",
       "                         'tvec__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
       "                         'tvec__stop_words': ['english']},\n",
       "             scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting Randomforest CV GS\n",
    "grid_search_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10,\n",
       "             estimator=Pipeline(steps=[('tvec', TfidfVectorizer()),\n",
       "                                       ('mb', MultinomialNB())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'tvec__ngram_range': [(1, 1), (1, 2)],\n",
       "                         'tvec__stop_words': ['english']},\n",
       "             scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting up GridSearch for TFIDFVectorizer\n",
    "grid_search_nb = GridSearchCV(nb_pipe, param_grid=tf_params,  verbose =1, n_jobs = -1, cv = 10, scoring = \"f1\")\n",
    "grid_search_nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.c. Assessing\n",
    "\n",
    "Use the testing data to measure the accuracy and F1-score of your model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 600 entries, 0 to 599\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Sentence  600 non-null    object\n",
      " 1   Polarity  600 non-null    int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 9.5+ KB\n",
      "None\n",
      "                                            Sentence  Polarity\n",
      "0  A good commentary of today's love and undoubte...         1\n",
      "1  For people who are first timers in film making...         1\n",
      "2  It was very popular when I was in the cinema, ...         1\n",
      "3  It's a feel-good film and that's how I felt wh...         1\n",
      "4  It has northern humour and positive about the ...         1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_df = pd.read_csv(path + \"sentiment_test.csv\")\n",
    "\n",
    "print(test_df.info())\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    good commentary today love undoubtedly film wo...\n",
      "1    people first timer film making think excellent...\n",
      "2    popular cinema good house good reaction plenty...\n",
      "3                      feel good film felt came cinema\n",
      "4        northern humour positive community represents\n",
      "Name: Sentence, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_test = test_df['Sentence']\n",
    "y_test = test_df['Polarity']\n",
    "X_test,y_test = preprocess(X_test,y_test)\n",
    "\n",
    "print(X_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lr = grid_search_lr.predict(X_test)\n",
    "pred_GB = grid_search_GB.predict(X_test)\n",
    "pred_rf = grid_search_rf.predict(X_test)\n",
    "pred_nb = grid_search_nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ LOGISTIC REGRESSION #################\n",
      "[[251  35]\n",
      " [100 213]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.88      0.79       286\n",
      "           1       0.86      0.68      0.76       313\n",
      "\n",
      "    accuracy                           0.77       599\n",
      "   macro avg       0.79      0.78      0.77       599\n",
      "weighted avg       0.79      0.77      0.77       599\n",
      "\n",
      "############ GRADIENT BOOSTING #################\n",
      "[[262  24]\n",
      " [148 165]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.92      0.75       286\n",
      "           1       0.87      0.53      0.66       313\n",
      "\n",
      "    accuracy                           0.71       599\n",
      "   macro avg       0.76      0.72      0.71       599\n",
      "weighted avg       0.76      0.71      0.70       599\n",
      "\n",
      "############ RANDOM FOREST #################\n",
      "[[259  27]\n",
      " [142 171]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.91      0.75       286\n",
      "           1       0.86      0.55      0.67       313\n",
      "\n",
      "    accuracy                           0.72       599\n",
      "   macro avg       0.75      0.73      0.71       599\n",
      "weighted avg       0.76      0.72      0.71       599\n",
      "\n",
      "############ Naive Bayes #################\n",
      "[[249  37]\n",
      " [ 97 216]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.87      0.79       286\n",
      "           1       0.85      0.69      0.76       313\n",
      "\n",
      "    accuracy                           0.78       599\n",
      "   macro avg       0.79      0.78      0.78       599\n",
      "weighted avg       0.79      0.78      0.78       599\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"############ LOGISTIC REGRESSION #################\")\n",
    "print(confusion_matrix(y_true = y_test, y_pred = pred_lr))\n",
    "class_names_lr = [str(x) for x in grid_search_lr.best_estimator_.classes_]\n",
    "print(classification_report(y_true = y_test, y_pred = pred_lr,target_names=class_names_lr))\n",
    "\n",
    "print(\"############ GRADIENT BOOSTING #################\")\n",
    "print(confusion_matrix(y_true = y_test, y_pred = pred_GB))\n",
    "class_names_GB = [str(x) for x in grid_search_GB.best_estimator_.classes_]\n",
    "print(classification_report(y_true = y_test, y_pred = pred_GB,target_names=class_names_GB))\n",
    "\n",
    "\n",
    "print(\"############ RANDOM FOREST #################\")\n",
    "print(confusion_matrix(y_true = y_test, y_pred = pred_rf))\n",
    "class_names_rf = [str(x) for x in grid_search_rf.best_estimator_.classes_]\n",
    "print(classification_report(y_true = y_test, y_pred = pred_rf,target_names=class_names_rf))\n",
    "\n",
    "\n",
    "print(\"############ Naive Bayes #################\")\n",
    "print(confusion_matrix(y_true = y_test, y_pred = pred_nb))\n",
    "class_names_nb = [str(x) for x in grid_search_nb.best_estimator_.classes_]\n",
    "print(classification_report(y_true = y_test, y_pred = pred_nb,target_names=class_names_nb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score:  0.798823009490657\n",
      "Best Params:  {'tvec__ngram_range': (1, 2), 'tvec__stop_words': 'english'}\n"
     ]
    }
   ],
   "source": [
    "print('Best Score: ', grid_search_nb.best_score_)\n",
    "print('Best Params: ', grid_search_nb.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8703947809379119\n",
      "0.763250883392226\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes\n",
    "prob_nb = grid_search_nb.predict_proba(X_test)\n",
    "# Create a prediction set:\n",
    "predictions = grid_search_nb.predict(X_test)\n",
    "from sklearn import metrics\n",
    "# Check AUC\n",
    "print(metrics.roc_auc_score(y_test,prob_lr[:, 1]))\n",
    "# Check F1 Score\n",
    "print(metrics.f1_score(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[249  37]\n",
      " [ 97 216]]\n",
      "AUC Score = 0.870\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.87      0.79       286\n",
      "           1       0.85      0.69      0.76       313\n",
      "\n",
      "    accuracy                           0.78       599\n",
      "   macro avg       0.79      0.78      0.78       599\n",
      "weighted avg       0.79      0.78      0.78       599\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#round results\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "#Get the confusion matrix\n",
    "cf_matrix = confusion_matrix(y_test,predictions)\n",
    "print(cf_matrix)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test,predictions).ravel()\n",
    "\n",
    "\n",
    "sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, \n",
    "            fmt='.2%', cmap='Blues')\n",
    "\n",
    "\n",
    "print(\"AUC Score = {:.3f}\".format(roc_auc_score(y_test,prob_nb[:, 1])))\n",
    "print()\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2. Given the accuracy and F1-score of your model, are you satisfied with the results, from a business point of view? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have tried Logistic Regression, Gradient Boosting, Random Forest, Naive Bayes models and of all these three models Naive Bayes performed well with a F1 score of 76%. The model didn’t overfit, however, we can improve the F1 score of the model by doing text augmentation, more data cleaning and feature engineering. My model has a decent accuracy (78%),an AUC score of 0.870, and good precision for positive class (1) of 0.85. We can adjust the thresholds and improve the recall by balancing the trade-off between recall and precision. \n",
    "\n",
    "Accuracy can be misleading sometimes and is not always a good indicator from a business perspective. Accuracy does not take into consideration the false negatives and false positives. Hence the F1 score is a best indicator of how well the model is performing. The F1 score is weighted average of precision and recall. Our precision is about 79% which is good. As shown in the confusion matrix above, we can see that true positives and true negatives are both relatively high at 216, 249 respectively whereas false negatives and false positives are relatively low. \n",
    "\n",
    "Since the dataset is balanced, the prediction in both classes was reasonably consistent. From a business perspective, predicting a negative review “Class 0” is more important as it can help the business take necessary actions. The best model in this case was able to predict class-0 with a True Positive rate of 0.87 which is beneficial to the business. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3. Show five example instances in which your model’s predictions were incorrect. Describe why you think the model was wrong. Don’t just guess: dig deep to figure out the root cause."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Insert your answer here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A good commentary of today's love and undoubte...</td>\n",
       "      <td>1</td>\n",
       "      <td>good commentary today love undoubtedly film wo...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.214999</td>\n",
       "      <td>0.785001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>For people who are first timers in film making...</td>\n",
       "      <td>1</td>\n",
       "      <td>people first timer film making think excellent...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.287188</td>\n",
       "      <td>0.712812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It was very popular when I was in the cinema, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>popular cinema good house good reaction plenty...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.306896</td>\n",
       "      <td>0.693104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It's a feel-good film and that's how I felt wh...</td>\n",
       "      <td>1</td>\n",
       "      <td>feel good film felt came cinema</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.375494</td>\n",
       "      <td>0.624506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It has northern humour and positive about the ...</td>\n",
       "      <td>1</td>\n",
       "      <td>northern humour positive community represents</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.390741</td>\n",
       "      <td>0.609259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  Polarity  \\\n",
       "0  A good commentary of today's love and undoubte...         1   \n",
       "1  For people who are first timers in film making...         1   \n",
       "2  It was very popular when I was in the cinema, ...         1   \n",
       "3  It's a feel-good film and that's how I felt wh...         1   \n",
       "4  It has northern humour and positive about the ...         1   \n",
       "\n",
       "                                            Sentence    0         0         1  \n",
       "0  good commentary today love undoubtedly film wo...  1.0  0.214999  0.785001  \n",
       "1  people first timer film making think excellent...  1.0  0.287188  0.712812  \n",
       "2  popular cinema good house good reaction plenty...  1.0  0.306896  0.693104  \n",
       "3                    feel good film felt came cinema  1.0  0.375494  0.624506  \n",
       "4      northern humour positive community represents  1.0  0.390741  0.609259  "
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data = pd.concat([test_df,X_test, pd.DataFrame(predictions),pd.DataFrame(prob_nb)],axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>proc_sentence</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Class_0_prob</th>\n",
       "      <th>Class_1_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A good commentary of today's love and undoubte...</td>\n",
       "      <td>1</td>\n",
       "      <td>good commentary today love undoubtedly film wo...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.214999</td>\n",
       "      <td>0.785001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>For people who are first timers in film making...</td>\n",
       "      <td>1</td>\n",
       "      <td>people first timer film making think excellent...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.287188</td>\n",
       "      <td>0.712812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It was very popular when I was in the cinema, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>popular cinema good house good reaction plenty...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.306896</td>\n",
       "      <td>0.693104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It's a feel-good film and that's how I felt wh...</td>\n",
       "      <td>1</td>\n",
       "      <td>feel good film felt came cinema</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.375494</td>\n",
       "      <td>0.624506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It has northern humour and positive about the ...</td>\n",
       "      <td>1</td>\n",
       "      <td>northern humour positive community represents</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.390741</td>\n",
       "      <td>0.609259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  Polarity  \\\n",
       "0  A good commentary of today's love and undoubte...         1   \n",
       "1  For people who are first timers in film making...         1   \n",
       "2  It was very popular when I was in the cinema, ...         1   \n",
       "3  It's a feel-good film and that's how I felt wh...         1   \n",
       "4  It has northern humour and positive about the ...         1   \n",
       "\n",
       "                                       proc_sentence  Prediction  \\\n",
       "0  good commentary today love undoubtedly film wo...         1.0   \n",
       "1  people first timer film making think excellent...         1.0   \n",
       "2  popular cinema good house good reaction plenty...         1.0   \n",
       "3                    feel good film felt came cinema         1.0   \n",
       "4      northern humour positive community represents         1.0   \n",
       "\n",
       "   Class_0_prob  Class_1_prob  \n",
       "0      0.214999      0.785001  \n",
       "1      0.287188      0.712812  \n",
       "2      0.306896      0.693104  \n",
       "3      0.375494      0.624506  \n",
       "4      0.390741      0.609259  "
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data.columns = ['Sentence','Polarity',\"proc_sentence\",\"Prediction\",\"Class_0_prob\",\"Class_1_prob\"]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>proc_sentence</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Class_0_prob</th>\n",
       "      <th>Class_1_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It really created a unique feeling though.</td>\n",
       "      <td>1</td>\n",
       "      <td>really created unique feeling though</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.591357</td>\n",
       "      <td>0.408643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I would have casted her in that role after rea...</td>\n",
       "      <td>1</td>\n",
       "      <td>would casted role ready script</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.794115</td>\n",
       "      <td>0.205885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The soundtrack wasn't terrible, either.</td>\n",
       "      <td>1</td>\n",
       "      <td>soundtrack terrible either</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.955162</td>\n",
       "      <td>0.044838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Still, it was the SETS that got a big \"10\" on ...</td>\n",
       "      <td>1</td>\n",
       "      <td>still set got big oy vey scale</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.742701</td>\n",
       "      <td>0.257299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The last 15 minutes of movie are also not bad ...</td>\n",
       "      <td>1</td>\n",
       "      <td>last minute movie also bad well</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.956210</td>\n",
       "      <td>0.043790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  Polarity  \\\n",
       "0       It really created a unique feeling though.           1   \n",
       "1  I would have casted her in that role after rea...         1   \n",
       "2          The soundtrack wasn't terrible, either.           1   \n",
       "3  Still, it was the SETS that got a big \"10\" on ...         1   \n",
       "4  The last 15 minutes of movie are also not bad ...         1   \n",
       "\n",
       "                          proc_sentence  Prediction  Class_0_prob  \\\n",
       "0  really created unique feeling though         0.0      0.591357   \n",
       "1        would casted role ready script         0.0      0.794115   \n",
       "2            soundtrack terrible either         0.0      0.955162   \n",
       "3        still set got big oy vey scale         0.0      0.742701   \n",
       "4       last minute movie also bad well         0.0      0.956210   \n",
       "\n",
       "   Class_1_prob  \n",
       "0      0.408643  \n",
       "1      0.205885  \n",
       "2      0.044838  \n",
       "3      0.257299  \n",
       "4      0.043790  "
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data with instances whcih are classified wrong\n",
    "data_set = data[((data[\"Polarity\"]==1) & (data[\"Prediction\"]==0)) | \n",
    "                    ((data[\"Polarity\"]==0) & (data[\"Prediction\"]==1)) ]\n",
    "\n",
    "data_set.reset_index(drop=True,inplace=True)\n",
    "data_set.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(177, 6)"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set.to_csv(path + 'data_set.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 177 number of predictions out of 600 rows of data that are classified wrongly by the Model.\n",
    "Looking at some instances, the model categorized the words like 'loose', 'terrible', 'boring' etc as negative words.\n",
    "Some of the words like 'Not' were removed because of applying stopwords. Hence the tone of the sentence was completely changed. We can improve the model performance by adding more features, and performing the sentiment analysis using TexBlob etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence         Not even good for camp value!  \n",
      "Polarity                                       0\n",
      "proc_sentence               even good camp value\n",
      "Prediction                                     1\n",
      "Prob_Class_0                            0.135654\n",
      "Prob_Class_1                            0.864346\n",
      "Name: 9, dtype: object\n",
      "***************Original Sentence***************\n",
      "Not even good for camp value!  \n"
     ]
    }
   ],
   "source": [
    "###List of incorrect predictions\n",
    "#Instance 1:\n",
    "print(data_analyse.loc[9,:])\n",
    "print(\"***************Original Sentence***************\")\n",
    "print(data_analyse.loc[9,\"Sentence\"])\n",
    "# Actual Polarity negative\n",
    "# Model predicted it as positive\n",
    "\n",
    "# Reason: \"Not\" was removed by stopwords which changed the tone of the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence         My 8/10 score is mostly for the plot.  \n",
      "Polarity                                               1\n",
      "proc_sentence                          score mostly plot\n",
      "Prediction                                             0\n",
      "Prob_Class_0                                    0.949322\n",
      "Prob_Class_1                                   0.0506785\n",
      "Name: 7, dtype: object\n",
      "***************Original Sentence***************\n",
      "My 8/10 score is mostly for the plot.  \n"
     ]
    }
   ],
   "source": [
    "##Instance 2:\n",
    "print(data_analyse.loc[7,:])\n",
    "print(\"***************Original Sentence***************\")\n",
    "print(data_analyse.loc[7,\"Sentence\"])\n",
    "# Actual Polarity positive\n",
    "# Model predicted it as negative\n",
    "\n",
    "# Reason: Since this is in context of movie rating, \n",
    "# numbers also determine the sentiment of the review, all the numbers are removed as part of pre processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence         The directing is sloppy at best.  \n",
      "Polarity                                          0\n",
      "proc_sentence                 directing sloppy best\n",
      "Prediction                                        1\n",
      "Prob_Class_0                               0.225054\n",
      "Prob_Class_1                               0.774946\n",
      "Name: 11, dtype: object\n",
      "***************Original Sentence***************\n",
      "The directing is sloppy at best.  \n"
     ]
    }
   ],
   "source": [
    "##Instance 3:\n",
    "print(data_analyse.loc[11,:])\n",
    "print(\"***************Original Sentence***************\")\n",
    "print(data_analyse.loc[11,\"Sentence\"])\n",
    "# Actual Polarity positive\n",
    "# Model predicted it as negative\n",
    "\n",
    "# Reason: The review consists of  sarcasm. \n",
    "# Since the word \"best\" is present in the review, the model predicted this as positive review. \n",
    "# The model is unable to differentiate sarcasm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence         Later I found myself lost in the power of the ...\n",
      "Polarity                                                         1\n",
      "proc_sentence                          later found lost power film\n",
      "Prediction                                                       0\n",
      "Prob_Class_0                                              0.677367\n",
      "Prob_Class_1                                              0.322633\n",
      "Name: 20, dtype: object\n",
      "***************Original Sentence***************\n",
      "Later I found myself lost in the power of the film.  \n"
     ]
    }
   ],
   "source": [
    "##Instance 4:\n",
    "print(data_analyse.loc[20,:])\n",
    "print(\"***************Original Sentence***************\")\n",
    "print(data_analyse.loc[20,\"Sentence\"])\n",
    "# Actual Polarity negative\n",
    "# Model predicted it as positive\n",
    "\n",
    "# Reason: The review contains phrase which means positive but the word \"lost\" has rendered negative \n",
    "# sentiment to the review. \n",
    "# Here we might add features like parts of speech \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence         This second appearance of Mickey Mouse (follow...\n",
      "Polarity                                                         1\n",
      "proc_sentence    second appearance mickey mouse following silen...\n",
      "Prediction                                                       0\n",
      "Prob_Class_0                                              0.709422\n",
      "Prob_Class_1                                              0.290578\n",
      "Name: 25, dtype: object\n",
      "***************Original Sentence***************\n",
      "This second appearance of Mickey Mouse (following the silent PLANE CRAZY earlier that year) is probably his most famous film--mostly because it was so ground-breaking.  \n"
     ]
    }
   ],
   "source": [
    "##Instance 5:\n",
    "print(data_analyse.loc[25,:])\n",
    "print(\"***************Original Sentence***************\")\n",
    "print(data_analyse.loc[25,\"Sentence\"])\n",
    "# Actual Polarity negative\n",
    "# Model predicted it as positive\n",
    "\n",
    "# Reason: The review although has some positive words, words like \"crazy\" and \"breaking\" have affected the sentiment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
